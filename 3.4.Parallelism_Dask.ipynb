{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5.2_Parallelism_complete.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTiZAM_7fjqM"
      },
      "source": [
        "# 1. Parallel Processing\n",
        "\n",
        "Let's explore how we can use *parallelism*, locally, to scale up big data.\n",
        "\n",
        "We'll start by looking at supporting multiple cores..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrGKf8udfw2W"
      },
      "source": [
        "## 1.1 Parallel dataframe processing with Dask\n",
        "\n",
        "The Dask library implements a subset of the Pandas API (and some others, such as Numpy) in a way that can run in multiple CPU threads (and thus on multiple cores).  It also supports certain cluster-based computations, although that won't be our focus.\n",
        "\n",
        "Let's start by installing Dask..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t43RR8z8c8Rn"
      },
      "source": [
        "!pip install dask[complete]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNpUyHTNdCc7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# JSON parsing\n",
        "import json\n",
        "\n",
        "import dask\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# HTML parsing\n",
        "from lxml import etree\n",
        "import urllib\n",
        "\n",
        "# SQLite RDBMS\n",
        "import sqlite3\n",
        "\n",
        "# Time conversions\n",
        "import time\n",
        "\n",
        "# NoSQL DB\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import DuplicateKeyError, OperationFailure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B9GLhpYdTiD"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1CtSFvqTM-JTxWu7-lfGYba1tLYkcqIZC',\n",
        "                                    dest_path='/content/linkedin_small.json.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTEOiklfddnP"
      },
      "source": [
        "%%time\n",
        "# 100K records from linkedin\n",
        "linked_in = open('/content/linkedin_small.json.txt')\n",
        "    \n",
        "people = []\n",
        "\n",
        "for line in linked_in:\n",
        "    person = json.loads(line)\n",
        "    people.append(person)\n",
        "    \n",
        "people_df = pd.DataFrame(people)\n",
        "print (\"%d records\"%len(people_df))\n",
        "\n",
        "people_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuZ83RmVdepY"
      },
      "source": [
        "people_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51d_3y95dkUK"
      },
      "source": [
        "skills_df = people_df[['_id','skills']].explode('skills')\n",
        "education_df = people_df[['_id','education']].explode('education')\n",
        "experience_df = people_df[['_id','experience']]\n",
        "honors_df = people_df[['_id', 'honors']]\n",
        "\n",
        "linkedin_df = people_df.copy().drop(columns=['skills','education','experience','honors'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aTX8hkQeVY2"
      },
      "source": [
        "linkedin_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbzjNbkPeXDm"
      },
      "source": [
        "%%time\n",
        "\n",
        "linkedin_df.merge(experience_df, on='_id').merge(skills_df, on='_id').merge(honors_df, on='_id').merge(education_df, on='_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0wYE2s3eiGi"
      },
      "source": [
        "linkedin_ddf = dd.from_pandas(linkedin_df,npartitions=100)\n",
        "skills_ddf = dd.from_pandas(skills_df,npartitions=100)\n",
        "experience_ddf = dd.from_pandas(experience_df,npartitions=100)\n",
        "education_ddf = dd.from_pandas(education_df,npartitions=100)\n",
        "honors_ddf = dd.from_pandas(honors_df, npartitions=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5cvm6APfBaL"
      },
      "source": [
        "%%time\n",
        "linkedin_ddf.merge(experience_ddf, on='_id').merge(skills_ddf, on='_id').merge(honors_ddf, on='_id').merge(education_ddf, on='_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkD3Wp8yfSJh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}